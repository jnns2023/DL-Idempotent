{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from idem_net_mnist import IdemNetMnist\n",
    "from idem_net_celeba import IdemNetCeleba\n",
    "from data_loader import load_MNIST, load_CelebA\n",
    "import matplotlib.pyplot as plt\n",
    "from plot_utils import plot_generation\n",
    "from ignite.metrics import FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_id = \"celeba20241113-154812\"\n",
    "epoch_num = \"_final.pth\"\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "checkpoint_path = f\"checkpoints/{run_id}/{epoch_num}\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"celeba\" in run_id:\n",
    "  model = IdemNetCeleba(3) # IdemNetMnist()\n",
    "else:\n",
    "  model = IdemNetMnist()\n",
    "\n",
    "state_dict = torch.load(checkpoint_path, weights_only=True, map_location=device)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of real images for FID score calculation (CelebA images)\n",
    "data_loader, test_loader = load_CelebA(batch_size=32)\n",
    "test_imgs, _ = next(iter(data_loader))\n",
    "test_imgs = test_imgs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "z_gen = torch.randn_like(test_imgs)  \n",
    "with torch.no_grad(): \n",
    "    f_z = model(z_gen) \n",
    "    ff_z = model(f_z)  \n",
    "\n",
    "# Normalize the images to [0, 1] for visualization\n",
    "z_gen = (z_gen - z_gen.min()) / (z_gen.max() - z_gen.min())\n",
    "f_z = (f_z - f_z.min()) / (f_z.max() - f_z.min())\n",
    "ff_z = (ff_z - ff_z.min()) / (ff_z.max() - ff_z.min())\n",
    "\n",
    "def plot_generated_images(z_gen, f_z, ff_z, device, num_imgs=3):\n",
    "    # Convert the tensors to NumPy arrays for visualization\n",
    "    z_gen_np = z_gen.cpu().numpy()\n",
    "    f_z_np = f_z.cpu().numpy()\n",
    "    ff_z_np = ff_z.cpu().numpy()\n",
    "\n",
    "    # If the images are in [batch_size, channels, height, width], transpose them to [batch_size, height, width, channels]\n",
    "    z_gen_np = np.transpose(z_gen_np, (0, 2, 3, 1))\n",
    "    f_z_np = np.transpose(f_z_np, (0, 2, 3, 1))\n",
    "    ff_z_np = np.transpose(ff_z_np, (0, 2, 3, 1))\n",
    "\n",
    "    # Create a plot with subplots (grid)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(8, 8))  # 3x3 grid for 9 images\n",
    "    axes = axes.ravel()  # Flatten the axes array\n",
    "\n",
    "    # Plot the random noise, f_z, and ff_z\n",
    "    for i in range(num_imgs):\n",
    "        # Plot z_gen (noise)\n",
    "        axes[i].imshow(z_gen_np[i])  # Display the noise image\n",
    "        axes[i].axis('off')  # Turn off axis labels\n",
    "        axes[i].set_title(f\"Noise {i+1}\")\n",
    "        \n",
    "\n",
    "    # Plot f_z\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(8, 8))  # 1x3 grid for 3 images\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i in range(num_imgs):\n",
    "        axes[i].imshow(f_z_np[i])  # Display the generated image\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"f_z {i+1}\")\n",
    "        \n",
    "\n",
    "    # Plot ff_z\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(8, 8))  # 1x3 grid for 3 images\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i in range(num_imgs):\n",
    "        axes[i].imshow(ff_z_np[i])  # Display the second set of generated images\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"ff_z {i+1}\")\n",
    "        \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_generated_images(z_gen, f_z, ff_z, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from ignite.metrics import FID\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.models.inception import inception_v3\n",
    "\n",
    "# Preprocessing function for images (resize and normalize)\n",
    "def preprocess_images(images, target_size=299):\n",
    "    # Resize using interpolate (for tensors)\n",
    "    images_resized = F.interpolate(images, size=(target_size, target_size), mode='bilinear', align_corners=False)\n",
    "    \n",
    "    # Normalize images to [-1, 1] (assuming input range is [0, 1])\n",
    "    images_normalized = (images_resized - 0.5) / 0.5  # Assuming the images are in range [0, 1]\n",
    "    \n",
    "    return images_normalized\n",
    "\n",
    "# Instantiate the InceptionV3 model for FID calculation\n",
    "def get_inception_model():\n",
    "    model = inception_v3(weights=True, transform_input=False)\n",
    "    model.eval()  # Set to eval mode for inference\n",
    "    return model\n",
    "\n",
    "# Function to compute FID score between f_z and ff_z\n",
    "def compute_fid(f_z, ff_z):\n",
    "    device = torch.device('cpu')  # Switch to CPU\n",
    "    f_z_preprocessed = preprocess_images(f_z).to(device)\n",
    "    ff_z_preprocessed = preprocess_images(ff_z).to(device)\n",
    "\n",
    "\n",
    "    inception_model = get_inception_model().to(device)\n",
    "\n",
    "    # Extract features from the generated images\n",
    "    f_z_features = inception_model(f_z_preprocessed)\n",
    "    ff_z_features = inception_model(ff_z_preprocessed)\n",
    "\n",
    "    # Compute FID score (Inception model will output activations, which we use)\n",
    "    fid_score = FID()(f_z_features, ff_z_features)\n",
    "    return fid_score\n",
    "\n",
    "# Example of usage: compute FID between f_z and ff_z\n",
    "fid_score = compute_fid(f_z, ff_z)\n",
    "\n",
    "# InceptionV3 model to extract features\n",
    "device = f_z.device()\n",
    "print(f\"FID score between f_z and ff_z: {fid_score.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.models import inception_v3\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "\n",
    "def preprocess_images(images, target_size=299):\n",
    "    # Resize using interpolate (for tensors)\n",
    "    images_resized = F.interpolate(images, size=(target_size, target_size), mode='bilinear', align_corners=False)\n",
    "    \n",
    "    # Normalize images to [-1, 1]\n",
    "    images_normalized = (images_resized - 0.5) / 0.5  # Assuming the images are in range [0, 1]\n",
    "    \n",
    "    return images_normalized\n",
    "\n",
    "\n",
    "def get_inception_model():\n",
    "    model = inception_v3(weights=\"IMAGENET1K_V1\", transform_input=False)\n",
    "    model.Mixed_7c.register_forward_hook(_hook_fn)  # Attach the hook to the last pooling layer\n",
    "    model.fc = torch.nn.Identity()  # Remove the classification head\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Hook function to capture the output of the last pooling layer\n",
    "def _hook_fn(module, input, output):\n",
    "    global pooled_features\n",
    "    pooled_features = output\n",
    "\n",
    "\n",
    "def extract_features(images, model):\n",
    "    global pooled_features\n",
    "    pooled_features = None  # Reset the global variable\n",
    "    \n",
    "    # Register the hook on the last pooling layer\n",
    "    last_pooling_layer = model.Mixed_7c  # InceptionV3's last pooling layer\n",
    "    hook_handle = last_pooling_layer.register_forward_hook(_hook_fn)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model(images)  # Forward pass to trigger the hook\n",
    "    \n",
    "    # Remove the hook after the forward pass\n",
    "    hook_handle.remove()\n",
    "    \n",
    "    # Flatten the features and return\n",
    "    return pooled_features.view(pooled_features.size(0), -1)  # Flatten the output\n",
    "\n",
    "\n",
    "# Used because the feature space is too big (TODO: consider doing batch wise instead of applying PCA)\n",
    "def reduce_dimensionality(features, n_components=None):\n",
    "    features_np = features.detach().cpu().numpy()\n",
    "    max_components = min(features_np.shape[0], features_np.shape[1])\n",
    "    if n_components is None or n_components > max_components:\n",
    "        n_components = max_components\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_features = pca.fit_transform(features_np)\n",
    "    return reduced_features\n",
    "\n",
    "# Get the mean and covariance of the feature space (last pooling layer)\n",
    "def compute_statistics(features):\n",
    "    reduced_features = reduce_dimensionality(features)\n",
    "    mu = np.mean(reduced_features, axis=0)\n",
    "    sigma = np.cov(reduced_features, rowvar=False)\n",
    "    return mu, sigma\n",
    "\n",
    "# Compute the Fréchet distance\n",
    "def frechet_distance(mu1, sigma1, mu2, sigma2):\n",
    "    diff = mu1 - mu2\n",
    "    covmean = sqrtm(sigma1 @ sigma2)\n",
    "    # Numerical stability: if covmean has imaginary values, take only the real part\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    return np.sum(diff**2) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "\n",
    "# Function to compute FID score between f_z and ff_z\n",
    "def compute_fid(f_z, ff_z):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Preprocess\n",
    "    f_z_preprocessed = preprocess_images(f_z).to(device)\n",
    "    ff_z_preprocessed = preprocess_images(ff_z).to(device)\n",
    "\n",
    "\n",
    "    inception_model = get_inception_model().to(device)\n",
    "\n",
    "    # Extract features using the last pooling layer\n",
    "    f_z_features = extract_features(f_z_preprocessed, inception_model)\n",
    "    ff_z_features = extract_features(ff_z_preprocessed, inception_model)\n",
    "\n",
    "    # Compute statistics\n",
    "    mu1, sigma1 = compute_statistics(f_z_features)\n",
    "    mu2, sigma2 = compute_statistics(ff_z_features)\n",
    "\n",
    "    # Calculate FID\n",
    "    fid_score = frechet_distance(mu1, sigma1, mu2, sigma2)\n",
    "    return fid_score\n",
    "\n",
    "# usage between two random tensors\n",
    "img_1 = torch.rand(16, 3, 64, 64)  # Replace with actual data\n",
    "img_2 = torch.rand(16, 3, 64, 64)  # Replace with actual data\n",
    "\n",
    "fid_score = compute_fid(img_1, img_2)\n",
    "print(f\"FID score between f_z and ff_z: {fid_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
