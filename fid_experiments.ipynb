{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amgcsa\\miniconda3\\envs\\KTHenv\\Lib\\site-packages\\ignite\\handlers\\checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from idem_net_mnist import IdemNetMnist\n",
    "from idem_net_celeba import IdemNetCeleba\n",
    "from data_loader import load_MNIST, load_CelebA\n",
    "import matplotlib.pyplot as plt\n",
    "from plot_utils import plot_generation\n",
    "from ignite.metrics import FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "run_id = \"celeba20241113-154812\"\n",
    "epoch_num = \"_final.pth\"\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "checkpoint_path = f\"checkpoints/{run_id}/{epoch_num}\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if \"celeba\" in run_id:\n",
    "  model = IdemNetCeleba(3) # IdemNetMnist()\n",
    "else:\n",
    "  model = IdemNetMnist()\n",
    "\n",
    "state_dict = torch.load(checkpoint_path, weights_only=True, map_location=device)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of real images for FID score calculation (CelebA images)\n",
    "data_loader, test_loader = load_CelebA(batch_size=256)\n",
    "test_imgs, _ = next(iter(data_loader))\n",
    "test_imgs = test_imgs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model.to(device)\n",
    "# Generate random noise and pass through the model\n",
    "z_gen = torch.randn_like(test_imgs)  # Random noise, adjust size as needed\n",
    "with torch.no_grad():  # No need to compute gradients during inference\n",
    "    f_z = model(z_gen)  # Generate fake images from the noise\n",
    "    ff_z = model(f_z)  # Generate second set of fake images\n",
    "\n",
    "# Normalize the images to [0, 1] for visualization\n",
    "z_gen = (z_gen - z_gen.min()) / (z_gen.max() - z_gen.min())\n",
    "f_z = (f_z - f_z.min()) / (f_z.max() - f_z.min())\n",
    "ff_z = (ff_z - ff_z.min()) / (ff_z.max() - ff_z.min())\n",
    "\n",
    "def plot_generated_images(z_gen, f_z, ff_z, device, num_imgs=3):\n",
    "    # Convert the tensors to NumPy arrays for visualization\n",
    "    z_gen_np = z_gen.cpu().numpy()\n",
    "    f_z_np = f_z.cpu().numpy()\n",
    "    ff_z_np = ff_z.cpu().numpy()\n",
    "\n",
    "    # If the images are in [batch_size, channels, height, width], transpose them to [batch_size, height, width, channels]\n",
    "    z_gen_np = np.transpose(z_gen_np, (0, 2, 3, 1))\n",
    "    f_z_np = np.transpose(f_z_np, (0, 2, 3, 1))\n",
    "    ff_z_np = np.transpose(ff_z_np, (0, 2, 3, 1))\n",
    "\n",
    "    # Create a plot with subplots (grid)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(8, 8))  # 3x3 grid for 9 images\n",
    "    axes = axes.ravel()  # Flatten the axes array\n",
    "\n",
    "    # Plot the random noise, f_z, and ff_z\n",
    "    for i in range(num_imgs):\n",
    "        # Plot z_gen (noise)\n",
    "        axes[i].imshow(z_gen_np[i])  # Display the noise image\n",
    "        axes[i].axis('off')  # Turn off axis labels\n",
    "        axes[i].set_title(f\"Noise {i+1}\")\n",
    "        \n",
    "\n",
    "    # Plot f_z\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(8, 8))  # 1x3 grid for 3 images\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i in range(num_imgs):\n",
    "        axes[i].imshow(f_z_np[i])  # Display the generated image\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"f_z {i+1}\")\n",
    "        \n",
    "\n",
    "    # Plot ff_z\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(8, 8))  # 1x3 grid for 3 images\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i in range(num_imgs):\n",
    "        axes[i].imshow(ff_z_np[i])  # Display the second set of generated images\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"ff_z {i+1}\")\n",
    "        \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_generated_images(z_gen, f_z, ff_z, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from ignite.metrics import FID\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.models.inception import inception_v3\n",
    "\n",
    "# Preprocessing function for images (resize and normalize)\n",
    "def preprocess_images(images, target_size=299):\n",
    "    # Resize using interpolate (for tensors)\n",
    "    images_resized = F.interpolate(images, size=(target_size, target_size), mode='bilinear', align_corners=False)\n",
    "    \n",
    "    # Normalize images to [-1, 1] (assuming input range is [0, 1])\n",
    "    images_normalized = (images_resized - 0.5) / 0.5  # Assuming the images are in range [0, 1]\n",
    "    \n",
    "    return images_normalized\n",
    "\n",
    "# Instantiate the InceptionV3 model for FID calculation\n",
    "def get_inception_model():\n",
    "    model = inception_v3(pretrained=True, transform_input=False)\n",
    "    model.eval()  # Set to eval mode for inference\n",
    "    return model\n",
    "\n",
    "# Function to compute FID score between f_z and ff_z\n",
    "def compute_fid(f_z, ff_z):\n",
    "    # Apply preprocessing (resize and normalize)\n",
    "    f_z_preprocessed = preprocess_images(f_z).to(device)\n",
    "    ff_z_preprocessed = preprocess_images(ff_z).to(device)\n",
    "\n",
    "    inception_model = get_inception_model().to(device)\n",
    "\n",
    "    # Extract features from the generated images\n",
    "    f_z_features = inception_model(f_z_preprocessed)\n",
    "    ff_z_features = inception_model(ff_z_preprocessed)\n",
    "\n",
    "    # Compute FID score (Inception model will output activations, which we use)\n",
    "    fid_score = FID()(f_z_features, ff_z_features)\n",
    "    return fid_score\n",
    "\n",
    "# Example of usage: compute FID between f_z and ff_z\n",
    "fid_score = compute_fid(f_z, ff_z)\n",
    "\n",
    "# InceptionV3 model to extract features\n",
    "device = f_z.device()\n",
    "print(f\"FID score between f_z and ff_z: {fid_score.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KTHenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
